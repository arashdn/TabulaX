import ast
import math
import os
import pathlib
import pickle
import pprint
import shutil
import re
import time
import nltk

import transformers.general
import transformers.basic
from util.JoinEval import JoinEval
from util.dataset import sample_data

from classifier.classifierutil import get_gold_label, get_gpt_label
from classifier import classifierutil

from transformers.numeric import get_numeric_function
from transformers.algorithmic import get_algorithmic_function
from transformers.general import get_bridge_values
from transformers.string import get_string_function

transformers.general.USE_TQDM = True
transformers.basic.USE_TQDM = True

import preprocessors

BASE_PATH = pathlib.Path(__file__).absolute().parent.parent.parent.absolute()
ED_CACHE_PATH = BASE_PATH / "cache/edit_distance/ed.pkl"


MODEL_NAME = "gpt-4o-2024-05-13"  # ["gpt-4o-2024-05-13", "gpt-4o-mini-2024-07-18", "llama3.1-8b"]
PROMPT_VERSION = "v001"
BASIC_PROMPT = False  # Ignore framework and directly prompt the LLM


EXAMPLE_SIZE = 5
EXAMPLE_SIZE_TYPE = "fixed"
MATCHING_TYPE = 'edit_dist'  # ["exact", "edit_dist"]
CLASSIFICATION_TYPE = 'golden'  # ['golden', gpt_classifier']

TEST_ON_ALL_DATA = False


# DS_PATH = str(BASE_PATH / "data/Datasets/AutoJoin")
# DS_PATH = str(BASE_PATH / "data/Datasets/FlashFill")
DS_PATH = str(BASE_PATH / "data/Datasets/DataXFormer")
DS_NAME = pathlib.PurePath(DS_PATH).name

class_str = "" if CLASSIFICATION_TYPE == "golden" else f"_{CLASSIFICATION_TYPE}"
if BASIC_PROMPT:
    class_str = "_BASIC-PROMPT"


model_str = MODEL_NAME.replace("-2024-05-13", "").replace("-2024-07-18", "")
OUTPUT_DIR = BASE_PATH / f"data/output{class_str}_{model_str}/{DS_NAME}_PIPELINE_{EXAMPLE_SIZE}sets_{MATCHING_TYPE}"


# Init edit distance cache
ed_cache_dict = None
is_ed_updated = [False]

if os.path.exists(ED_CACHE_PATH):
    with open(ED_CACHE_PATH, 'rb') as fp:
        ed_cache_dict = pickle.load(fp)
else:
    ed_cache_dict = {}



def _save_ed_cache():
    if not is_ed_updated[0]:
        return
    with open(ED_CACHE_PATH, 'wb') as fp:
        pickle.dump(ed_cache_dict, fp)
    is_ed_updated[0] = False


def edit_distance_func(val1, val2):
    tup = (val1, val2)
    if tup not in ed_cache_dict:
        ed_cache_dict[tup] = nltk.edit_distance(val1, val2)
        is_ed_updated[0] = True
    return ed_cache_dict[tup]



def get_classes(method, tbl_name, ds_path):
    if method == "golden":
        return get_gold_label(tbl_name, ds_path)
    elif method == "gpt_classifier":
        return get_gpt_label(tbl_name, ds_path)
    else:
        raise NotImplementedError(f"Classification Method {method} not implemented")



def get_function_from_string(function_string):
    # """Generated By ChatGPT"""
    # Parse the function string into an abstract syntax tree (AST)

    try:
        parsed_ast = ast.parse(function_string)
    except SyntaxError:
        import sys
        print("Could not parse function string", file=sys.stderr)
        parsed_ast = ast.parse("def error_res(s): return s")

    # Extract the function definition node
    function_def_node = next((node for node in ast.walk(parsed_ast) if isinstance(node, ast.FunctionDef)), None)

    if function_def_node:
        # Compile the AST into a code object
        code_object = compile(parsed_ast, filename='<string>', mode='exec')

        # Execute the code object in a specific namespace
        namespace = {}
        exec(code_object, namespace)

        # Return the function from the namespace
        return namespace.get(function_def_node.name)

    return None


def extract_functions_lines_with_body(input_string):
    # """Generated By ChatGPT"""
    # Define a regular expression pattern to match Python function definitions and their bodies
    pattern = re.compile(r'(def\s+[a-zA-Z_][a-zA-Z0-9_]*\s*\([^)]*\):[^\n]*((?:\n[ \t]+.*)*)*)')

    # Find all matches in the input string
    matches = pattern.findall(input_string)
    return [match[0] for match in matches]



def get_transformations(funcs_list):

    funcs_str = extract_functions_lines_with_body(funcs_list)
    return [get_function_from_string(f) for f in funcs_str], funcs_str



def join(rows, transformation, matching_type):
    # return [("", "")], [{'inp': "inp", 'gen': "res", 'exp': "out"}]

    sources = []
    targets = []

    for row in rows:
        inp, out = row[0], row[1]
        sources.append(inp)
        targets.append(out)


    joins = []
    predicts = []

    for row in rows:
        inp, out = row[0], row[1]
        try:
            val = transformation(inp)
        except Exception as e:
            val = None

        if matching_type == 'edit_dist':
            min_dist = 100000
            min_key = None
            for target in targets:
                val = "" if val is None else val
                if type(val) is not str:
                    val = str(val)
                dist = edit_distance_func(target, val)
                if dist < min_dist:
                    min_dist = dist
                    min_key = target

            res, pred = min_key, val

        elif matching_type == 'exact':
            res, pred = (val, val) if val in targets else (None, None)

        elif matching_type == "num_dist":
            min_dist = float("inf")
            min_key = None
            for target in targets:
                # val = "" if val is None else val
                dist = abs(float(target) - val)
                if dist < min_dist:
                    min_dist = dist
                    min_key = target

            res, pred = str(min_key), str(val)

        else:
            raise Exception("Wrong matching type.")


        if pred is None:
            pred = ""
        predicts.append({'inp': inp, 'gen': res, 'exp': out, 'pred': pred})
        if res is not None:
            joins.append((str(inp), str(res)))


    return joins, predicts


def preprocess(table, pipline):
    test = [(src, tar, src) for src, tar in table['test']]
    train = [(src, tar, src) for src, tar in table['train']]

    details = []

    for process in pipline:
        func = getattr(preprocessors, process)
        train, test, detail = func(train, test, {
            'model_name': MODEL_NAME,
            'prompt_version': PROMPT_VERSION,
        })
        details.append(detail)


    return {
        'train': train,
        'test': test
    }, details



def main():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

    out_file_path = OUTPUT_DIR / "_res.csv"

    tables = sample_data(DS_PATH, EXAMPLE_SIZE, EXAMPLE_SIZE_TYPE)


    f = open(out_file_path, 'w')

    title_str = "id,class,P,R,F1,correct,len,gen_functions,avg_edit_dist,avg_norm_edit_dist,Time"
    print(title_str)
    i = 1
    if f is not None:
        print(title_str, file=f)

    for table in tables:
        start_time = time.time()

        print(f"{i}/{len(tables)}: {table}", end=',')
        i += 1

        cls = get_classes(CLASSIFICATION_TYPE, table, DS_PATH)
        assert cls in classifierutil.ALLOWED_CLASSES

        if BASIC_PROMPT:
            pre_pipeline = ["get_basic_bridge"]
        elif cls == "Numbers":
            pre_pipeline = ["parse_numbers"]
        elif cls == "General":
            pre_pipeline = ["get_bridge_table"]
        else:
            pre_pipeline = []
        new_table, preprocess_details = preprocess(tables[table], pre_pipeline)
        tables[table] = new_table

        # (bridge, target, src)

        train = tables[table]['train']
        test = tables[table]['test']
        if TEST_ON_ALL_DATA:
            test = test + tables[table]['train']

        matching_type = MATCHING_TYPE

        if cls == "General" or BASIC_PROMPT:
            #  Most of the work is done in preprocessing
            details, funcs_list = {}, "def transform_source_to_target1(value): return value"
        elif cls == "Numbers":
            details, funcs_list = {}, get_numeric_function(train)
            matching_type = "num_dist"
        elif cls == "Algorithmic":
            details, funcs_list = get_algorithmic_function(train, model_name=MODEL_NAME, prompt_version=PROMPT_VERSION)
            matching_type = "exact"
        elif cls == "String":
            details, funcs_list = get_string_function(train, model_name=MODEL_NAME, prompt_version=PROMPT_VERSION)
        else:
            raise NotImplementedError()



        funcs, funcs_str = get_transformations(funcs_list)

        details['funcs_str'] = funcs_str

        if len(funcs) > 0:
            joins, predicts = join(test, funcs[0], matching_type)
        else:
            joins, predicts = [], []
        avg_edit_dist = sum(
            edit_distance_func(p['pred'], p['exp']) for p in predicts
        ) / len(predicts) if len(predicts) != 0 else math.nan

        avg_norm_edit_dist = sum(
            edit_distance_func(p['pred'], p['exp']) / max(len(p['pred']), len(p['exp'])) for p in predicts
        ) / len(predicts) if len(predicts) != 0 else 1

        correct_cnt = sum(1 if p['pred'] == p['exp'] else 0 for p in predicts)

        test_strs = [(str(t[0]), t[1], t[2]) for t in test]
        je = JoinEval(joins, test_strs)

        taken_time = time.time() - start_time
        # print(str(je.short_str())+","+str(avg_edit_dist)+","+str(taken_time))
        val_str = f"{cls},{je.short_str()},{correct_cnt},{len(predicts)},{len(details['funcs_str'])},{avg_edit_dist},{avg_norm_edit_dist},{taken_time}"
        print(val_str)

        if f is not None:
            print(f"{table},{val_str}", file=f)

        with open(OUTPUT_DIR / f"{table}.txt", 'w') as f2:
            print(f"P, R, F = {je.short_str()}", file=f2)
            print(f"Class: {cls}", file=f2)
            print(f"Correct Predictions: {correct_cnt}", file=f2)
            print(f"Input Count: {len(predicts)}", file=f2)
            print(f"Average Edit Distance: {avg_edit_dist}", file=f2)
            print(f"Average Normalized Edit Distance: {avg_norm_edit_dist}", file=f2)
            print(f"Taken Time: {taken_time}", file=f2)
            print(f"Generated Functions: {len(details['funcs_str'])}", file=f2)
            print(f"Details: {details}", file=f2)
            # print(f"Prompt: {details['prompt']}", file=f2)
            print(f"Preprocessing details: {pprint.pformat(preprocess_details, indent=1, width=120)}", file=f2)
            print("\n\n===========\n\n", file=f2)
            print(f"Table (processed src, target, src): {pprint.pformat(tables[table], indent=1, width=120)}", file=f2)

        _save_ed_cache()

    f.close()


if __name__ == "__main__":
    main()
